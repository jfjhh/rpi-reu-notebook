\documentclass[14pt,c]{beamer}

\usepackage{ulem}

\usecolortheme{owl}
\setbeamercolor{frametitle}{bg=black}
\setbeamertemplate{frametitle}[default][center]
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\setbeamercolor{section in toc}{fg=white}
% \beamertemplatenavigationsymbolsempty%
\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[white]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber%
}

\usepackage[sorting=none,style=nature,doi=true,url=false]{biblatex}
\addbibresource{references.bib}
\renewcommand*{\bibfont}{\footnotesize}

% Fonts
\usefonttheme{professionalfonts} % So beamer won't change fonts
\usepackage{fontspec}
\setmainfont[Numbers={Lining,OldStyle}]{Linux Libertine}
\setsansfont{Linux Biolinum}
\usepackage[small,euler-digits]{eulervm} % For beamer slides
% \usepackage[libertine,vvarbb]{newtxmath}
\usepackage{realscripts}
\usepackage[scr=esstix,cal=boondoxo]{mathalfa}
\usepackage{bm} % Load after math font configuration
\setmonofont[Scale=MatchLowercase]{Iosevka}

% use microtype if available
\IfFileExists{microtype.sty}{\usepackage{microtype}}{}
\hypersetup{%
            pdftitle={Towards the entropy of images},
            pdfauthor={Alex Striff},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same} % don't use monospace font for urls

% \setlength{\parindent}{0pt}
% \setlength{\parskip}{6pt plus 2pt minus 1pt}
% \setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}

\usepackage{physics}
\usepackage{siunitx}
\usepackage{tikz}
\usetikzlibrary{arrows}
\tikzset{% Animate with beamer overlays
  invisible/.style={opacity=0},
  visible on/.style={alt={#1{}{invisible}}},
  alt/.code args={<#1>#2#3}{%
    \alt<#1>{\pgfkeysalso{#2}}{\pgfkeysalso{#3}} % \pgfkeysalso doesn't change the path
  },
}

\graphicspath{{./figs/}}

\title{How to know what you don't see}
\subtitle{Quantifying visual information}
% \subtitle{\textsc{Nsf} \textsc{rpi} \textsc{reu} midterm presentation}
\author{Alex Striff \\ Advisor: Vincent Meunier}
\institute{Reed College}
\date{July 8, 2020}

\begin{document}
\frame{\titlepage}

\begin{frame}{Outline}
  \centering
  \tableofcontents
\end{frame}

\section{What are we after?}

\begin{frame}{What do you see?}
  \centering
  \includegraphics[angle=1]{rect-lchnoise}
  % \\
  % \begin{minipage}[c]{0.75\textwidth}
  %   \centering
  %   \vspace{\baselineskip}
  %   \begin{itemize}
  %     \item Noumena and the irrelevant
  %     \item Phenomena and qualia
  %   \end{itemize}
  % \end{minipage}
\end{frame}

\section{Information and entropy}

\begin{frame}
  \onslide<+->
  Information from a random variable $X$ being $x \in \mathcal{X}$?
  \onslide<+->
  \begin{itemize}
    \item If $P(x) = 1$, $I(x) = 0$
    \item If $P(x) < P(x')$, $I(x) > I(x')$
    \item For independent events $x$ and $y$, $I(x, y) = I(x) + I(y)$
  \end{itemize}
  \onslide<+->
  \begin{align*}
    I(x)
    &= -\log P(x) \\
    \\
    \action<+->{S(X) &= \ev{I}_X \\}
    \action<.->{&= -\sum_{x \in \mathcal{X}} P(x) \log P(x)}
  \end{align*}
  \onslide<1->
  \scriptsize
  \fullcite{shannon1948mathematical}
\end{frame}

\begin{frame}{The maximum entropy principle}
  \begin{minipage}[c][0.75\textheight]{\textwidth}
    How to assign probabilities $p(x)$ to a random variable $X$ with given
    $\ev{\vb{f}(x)}$ in an unbiased way?
    \vspace{0.5\baselineskip}
    \begin{itemize}
      \item Maximize the entropy subject to moment constraints
      \item For mean energy, this gives the canonical distribution
      \item Subjective and objective probabilities
    \end{itemize}
    \vspace{\baselineskip}
  \end{minipage}
  \vfill
  \scriptsize
  \fullcite{jaynes1957information}
\end{frame}

\section{Image fluctuations}

\begin{frame}{Images as discrete-level systems}
  \onslide<+->
  \tikzset{%
    newcontent/.style = {blue,visible on=<.->}
  }
  \centering
  \begin{tikzpicture}[scale=0.8, every node/.style={transform shape}]
    \node[left] at (1.25*1, 1 + 0.5) {\small$1$};
    \node[left] at (1.25*1, 5 + 0.5) {\small$M$};
    \node[below] at (1.25*1 + 0.5,1) {\small$1$};
    \node[below] at (1.25*4 + 0.5,1) {\small$N$};
    \fill[gray] (1.25*1,4) rectangle ++(1,1);
    \fill[blue,visible on=<+->] (1.25*1,3) rectangle ++(1,1);
    \fill[gray] (1.25*2,1) rectangle ++(1,1);
    \fill[newcontent] (1.25*2,5) rectangle ++(1,1);
    \fill[gray] (1.25*4,2) rectangle ++(1,1);
    \fill[newcontent] (1.25*4,4) rectangle ++(1,1);
    \draw[newcontent] (1.25*4 + 1, 2 + 0.5) -- ++(0.75,0);
    \draw[newcontent] (1.25*4 + 1, 4 + 0.5) -- ++(0.75,0);
    \draw[newcontent,<->,thick] (1.25*4 + 1.5, 2 + 0.5) -- ++(0,2)
      node[midway,right]{$\Delta E_N = 2$};
    \node (ellipses) at (1.25*3 + 0.5, 3 + 0.5) {\Large$\cdots$};
    \foreach \x in {1,2,4} {%
      \foreach \y in {1,...,5} {%
        \draw (1.25*\x,\y) rectangle ++(1,1);
      }
    }
  \end{tikzpicture}
  \\
  \onslide<.->
  \textcolor{blue}{$E = \sum_i \Delta E_i$}
  \onslide<+->
  \[
    \begin{tikzpicture}[baseline={([yshift=-0.5ex]current bounding box.center)},
      scale=0.25, every node/.style={transform shape}]
      \fill[blue] (1.25*1,3) rectangle ++(1,1);
      \fill[blue] (1.25*2,2) rectangle ++(1,1);
      \fill[blue] (1.25*4,4) rectangle ++(1,1);
      \node (ellipses) at (1.25*3 + 0.5, 3 + 0.5) {\Large$\cdots$};
      \foreach \x in {1,2,4} {%
        \fill[gray] (1.25*\x,1) rectangle ++(1,1);
        \foreach \y in {1,...,5} {%
          \draw (1.25*\x,\y) rectangle ++(1,1);
        }
      }
    \end{tikzpicture}
    \quad
    \action<+->{%
      g(E) = \sum_k {(-1)}^k \binom{N}{k}\binom{N + E - kM - 1}{E - kM}
    }
  \]
\end{frame}

\section{Wang-Landau algorithm results}

\begin{frame}{Wang-Landau algorithm results}
  \centering
  \includegraphics[width=0.9\framewidth]{wanglandau-bw}
  \\
  \small
  \textcolor{white}{BW:\@ Black/white (exact)} \;
  \textcolor{blue}{WL:\@ Wang-Landau result}
\end{frame}

\begin{frame}{Wang-Landau algorithm results}
  \centering
  \includegraphics[width=0.9\framewidth]{wanglandau-bw-relerror}
  \\
  \small
  \textcolor{white}{Black image simulations} \;
  \textcolor{blue}{\uline{Mean} $\pm$ \dashuline{standard deviation}}
\end{frame}

\begin{frame}{Wang-Landau algorithm results}
  \centering
  \includegraphics[width=0.9\framewidth]{wanglandau-gray}
  \\
  \small
  \textcolor{white}{Random gray image simulations} \;
  \textcolor{blue}{BW:\@ Black/white (exact)}
\end{frame}

\begin{frame}{Wang-Landau algorithm results}
  \centering
  \includegraphics[width=0.9\framewidth]{wanglandau-gray-S}
  \\
  \small
  \textcolor{white}{Random gray image simulations} \;
  \textcolor{blue}{BW:\@ Black/white (exact)}
\end{frame}

\section{Other approaches taken}

\begin{frame}{Other approaches taken}
  \begin{itemize}
    \item All light entering the eye (receptive fields)
    \item Entropy for continuous coordinates (KL divergences)
  \end{itemize}
\end{frame}

\section{Next steps}

\begin{frame}{Next steps}
  \begin{itemize}
    \item Include spatial information (infer from neighbors)
    \item Include color (CIE Lab)
    \item Information as how to draw the image (Kolmogorov complexity)
  \end{itemize}
\end{frame}

\begin{frame}{Acknowledgements}
  \centering
  \includegraphics[width=1in]{nsf}
  \quad
  \includegraphics[width=1in]{rpi}
\end{frame}

\appendix

\begin{frame}[c]
  \begin{center}
  \Large
  Questions?
  \end{center}
\end{frame}

\begin{frame}{Appendix}
  \centering
  \tableofcontents
\end{frame}

\section{Intensity entropy}

\begin{frame}{Intensity entropy}
  \centering
  \includegraphics[width=0.75\framewidth]{intensity_entropy}
  \\
  \textcolor{white}{Original}\;
  \textcolor{red}{$5\times5$ IE}\;
  \textcolor{yellow}{$5\times5$ SD}\;
  \textcolor{green}{$41\times41$ IE}

  \small
  \textcolor{gray}{IE:\@ Intensity entropy}

  \textcolor{gray}{SD:\@ Standard deviation}
\end{frame}

\section{Inference and the maximum entropy method}

\begin{frame}{Inference}
  \begin{itemize}
    \item A noisy measurement of an image ($I$) produces data ($D$).
  \end{itemize}
  \centerline{Bayes' rule}
  \[
    \textcolor{green}{P(I \mathbin{|} D)}
    = \frac{\textcolor{yellow}{P(D \mathbin{|} I)}
      \textcolor{red}{%
    P(I)}}{\textcolor{blue}{P(D)}}
  \]
  \begin{itemize}
    \item We infer the \textcolor{green}{posterior} from the
      \textcolor{yellow}{likelihood} and \textcolor{red}{prior}, and
      normalize by the \textcolor{blue}{evidence}.
    \item \textsc{Map} estimate: maximize $\ln P(D \mathbin{|} I) + \ln
      P(I)$
    \item Maximum entropy method: $\textcolor{red}{P(I)} =
      e^{\lambda S(I)}$
  \end{itemize}
\end{frame}

\begin{frame}
  \centering
  \includegraphics[width=0.625\framewidth]{astro-maxent}
  \\
  \textcolor{white}{Original}\;
  \textcolor{red}{``Measured''}\;
  \textcolor{yellow}{\textsc{Mem}}\;
  \textcolor{green}{Multiscale \textsc{mem}}

  \scriptsize
  \vspace{\baselineskip}
  \fullcite{pantin1996deconvolution}
\end{frame}

\section{Information dimension}

\begin{frame}
  How to assign an \emph{information dimension} to the random variable $X$?
  \begin{align*}
    X_m &= \frac{\lfloor m X \rfloor}{m} \\
    d(X) &= \lim_{m \to \infty} \frac{S(X_m)}{\log m}
  \end{align*}
  \begin{itemize}
    \item \emph{Lebesgue decomposition theorem:} $d(X)$ is the fraction of
      $P_X$ that is discrete.

    \item For a $n$-vector $X$ with finite $H(\lfloor X \rfloor)$, $0 \le d(X)
      \le n$.
  \end{itemize}
\end{frame}

\section{References}

\begin{frame}
  \frametitle<presentation>{References}
  \nocite{*}
  \printbibliography%
\end{frame}

\end{document}

